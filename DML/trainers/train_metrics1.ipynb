{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter nbconvert --to script train_metrics1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the root directory: one level up from \"trainers\"\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(ROOT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# Import PyTorch Metric Learning\n",
    "from pytorch_metric_learning import losses, miners, reducers, distances\n",
    "from pytorch_metric_learning.utils import accuracy_calculator\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2 #np.array -> torch.tensor (B, 3, H, W)\n",
    "import timm\n",
    "\n",
    "from utils.dataset import TripletDataset\n",
    "from models.CNN_model import EmbeddingModel\n",
    "from utils.tools import AverageMeter\n",
    "from utils.warmup import GradualWarmupSchedulerV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "root_dir = '../datasets/'\n",
    "csv_train_file = 'train_data_with_folds.csv'\n",
    "class_list = ['normal', 'preplus', 'plus']\n",
    "label_dict = {cls: i for i, cls in enumerate(class_list)}\n",
    "\n",
    "df = pd.read_csv(os.path.join(root_dir, csv_train_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(image_size):\n",
    "    transforms_train = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        # A.ImageCompression(quality_lower=80, quality_upper=100, p=0.25),\n",
    "        # A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.5),\n",
    "        # A.Flip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.RandomBrightnessContrast(p=0.5),\n",
    "        # A.CoarseDropout(num_holes_range=(1,1), hole_height_range=(8, 32), hole_width_range=(8, 32), p=0.25),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    transforms_val = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    return transforms_train, transforms_val\n",
    "\n",
    "class UnNormalize(object):\n",
    "  def __init__(self, mean, std):\n",
    "    self.mean = mean\n",
    "    self.std = std\n",
    "\n",
    "  def __call__(self, tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      tensor (Tensor): Tensor image of size (C, H, W) to be normalized'\n",
    "    Returns:\n",
    "      Tensor: Normalized image\n",
    "    \"\"\"\n",
    "    for t, m, s in zip(tensor, self.mean, self.std):\n",
    "      t.mul_(s).add_(m)\n",
    "      #The normalize code -> t.sub_(m).div_(s)\n",
    "    return tensor\n",
    "\n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "\n",
    "def get_sampler(dataset): # WeightedRandomSampler\n",
    "    labels = [dataset[idx][1] for idx in range(len(dataset))]\n",
    "    class_counts = np.bincount(labels, minlength=num_classes)\n",
    "    class_weights = 1.0 / (class_counts + 1e-6)\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    return sampler\n",
    "\n",
    "\n",
    "# Plotting function\n",
    "def plot_fold_history(fold, history, run_dir):\n",
    "    epochs = list(range(1, len(history['train_loss']) + 1))\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot auc\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_auc'], '-o', label='Train AUC', color='skyblue')\n",
    "    plt.plot(epochs, history['val_auc'], '-o', label='Val AUC', color='lightcoral')\n",
    "    plt.scatter(history['best_val_auc_epoch'], history['best_val_auc'], s=200, color='lightcoral')\n",
    "    plt.text(history['best_val_auc_epoch'], history['best_val_auc'], f'max {history[\"best_val_auc\"]:.4f}', size=12)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title(f'Fold {fold} Auc')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_loss'], '-o', label='Train Loss', color='skyblue')\n",
    "    plt.plot(epochs, history['val_loss'], '-o', label='Val Loss', color='lightcoral')\n",
    "    plt.plot(epochs, history['triplet_loss'], '-o', label='Triplet Loss', color='green')\n",
    "    plt.scatter(history['best_val_loss_epoch'], history['best_val_loss'], s=200, color='lightcoral')\n",
    "    plt.text(history['best_val_loss_epoch'], history['best_val_loss'], f'min {history[\"best_val_loss\"]:.4f}', size=12)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {fold} Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(run_dir, f\"fold{fold}_history.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"History plot saved: {plot_path}\")\n",
    "    plt.show()\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, min_delta=0, patience=1, use_loss=False):\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.use_loss = use_loss\n",
    "        self.best_metric = float('inf') if use_loss else -float('inf')\n",
    "        self.count = 0\n",
    "        \n",
    "    def early_stop(self, metric):\n",
    "        if self.use_loss:\n",
    "            if metric < self.best_metric:\n",
    "                self.best_metric = metric\n",
    "                self.count = 0\n",
    "            elif metric > self.best_metric + self.min_delta:\n",
    "                self.count += 1\n",
    "                if self.count >= self.patience:\n",
    "                    return True\n",
    "        else:\n",
    "            if metric > self.best_metric:\n",
    "                self.best_metric = metric\n",
    "                self.count = 0\n",
    "            elif metric + self.min_delta < self.best_metric:\n",
    "                self.count += 1\n",
    "                if self.count >= self.patience:\n",
    "                    return True\n",
    "        return False\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(embeddings, labels, k=1):\n",
    "    \"\"\"\n",
    "    Compute Recall@K for given embeddings and labels.\n",
    "    \"\"\"\n",
    "    embeddings = np.asarray(embeddings)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # Fit Nearest Neighbors (exclude self-match)\n",
    "    nn_model = NearestNeighbors(n_neighbors=k + 1, metric='cosine')\n",
    "    nn_model.fit(embeddings)\n",
    "    distances, indices = nn_model.kneighbors(embeddings)\n",
    "\n",
    "    # Count correct labels among nearest neighbors (exclude self-match at idx 0)\n",
    "    correct = 0\n",
    "    for i in range(len(labels)):\n",
    "        neighbor_idxs = indices[i][1:]  # exclude self\n",
    "        neighbor_labels = labels[neighbor_idxs]\n",
    "        if labels[i] in neighbor_labels:\n",
    "            correct += 1\n",
    "\n",
    "    recall_at_k = correct / len(labels)\n",
    "    return recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, triplet_loss, miner, device, loss_type):\n",
    "    train_loss_meter = AverageMeter()\n",
    "    triplet_loss_meter = AverageMeter()\n",
    "    ce_loss_meter = AverageMeter()\n",
    "    model.train()\n",
    "    \n",
    "    PROBS = []\n",
    "    TARGETS = []\n",
    "    \n",
    "    for img, label in tqdm(loader, desc='Training'):\n",
    "      \n",
    "        optimizer.zero_grad()\n",
    "        inputs = img.to(device)\n",
    "        targets = label.to(device)\n",
    "        \n",
    "        # Get both logits and embeddings\n",
    "        logits, embeddings = model(inputs)\n",
    "        \n",
    "        # Calculate classification loss\n",
    "        ce_loss = criterion(logits, targets)\n",
    "        \n",
    "        hard_pairs = miner(embeddings, targets)\n",
    "        if len(hard_pairs[0]) == 0:\n",
    "            trip_loss = torch.tensor(0.0, device=device)\n",
    "        else:\n",
    "            trip_loss = triplet_loss(embeddings, targets, hard_pairs)\n",
    "\n",
    "        if loss_type == \"ce_only\":\n",
    "            loss = ce_loss\n",
    "        elif loss_type == \"triplet_only\":\n",
    "            loss = trip_loss\n",
    "        else:\n",
    "            loss = 0.5 * ce_loss + 0.5 * trip_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_meter.update(loss.item(), inputs.size(0))\n",
    "        ce_loss_meter.update(ce_loss.item(), inputs.size(0))\n",
    "        triplet_loss_meter.update(trip_loss.item() if trip_loss != 0 else 0, inputs.size(0))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs = F.softmax(logits.float(), dim=1).cpu().numpy()\n",
    "            PROBS.append(probs)\n",
    "            TARGETS.append(targets.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    PROBS = np.concatenate(PROBS)\n",
    "    TARGETS = np.concatenate(TARGETS)\n",
    "\n",
    "    if not np.allclose(PROBS.sum(axis=1), 1.0, atol=1e-5):\n",
    "        print(\"PROBS not summing to 1!\")\n",
    "    if np.any(TARGETS < 0) or np.any(TARGETS >= num_classes):\n",
    "        print(f\"Invalid TARGETS values: {TARGETS}\")\n",
    "\n",
    "    # Compute AUC over entire epoch\n",
    "    try:\n",
    "        train_auc = roc_auc_score(y_true=TARGETS, y_score=PROBS, multi_class='ovr')\n",
    "    except ValueError as e:\n",
    "        print(f\"Sample of PROBS: {PROBS[0]}, sum: {PROBS.sum(axis=1)}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        train_auc = 0.0\n",
    "        \n",
    "    return train_loss_meter.avg, train_auc, triplet_loss_meter.avg, ce_loss_meter.avg\n",
    "\n",
    "# Validation epoch\n",
    "def val_epoch(model, loader, criterion, triplet_loss, miner, device, loss_type):\n",
    "    model.eval()\n",
    "    val_loss_meter = AverageMeter()\n",
    "    triplet_loss_meter = AverageMeter()\n",
    "    ce_loss_meter = AverageMeter()\n",
    "    \n",
    "    PROBS = []\n",
    "    TARGETS = []\n",
    "    EMBEDDINGS = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in loader:\n",
    "            inputs = img.to(device)\n",
    "            targets = label.to(device)\n",
    "\n",
    "            logits, embeddings = model(inputs)\n",
    "            \n",
    "            # Calculate classification loss\n",
    "            ce_loss = criterion(logits, targets)\n",
    "            trip_loss = triplet_loss(embeddings, targets)\n",
    "            \n",
    "            if loss_type == \"ce_only\":\n",
    "                loss = ce_loss\n",
    "            elif loss_type == \"triplet_only\":\n",
    "                loss = trip_loss\n",
    "            else:\n",
    "                # print(\"CE and trip\")\n",
    "                loss = 0.5 * ce_loss + 0.5 * trip_loss\n",
    "            \n",
    "            val_loss_meter.update(loss.item(), inputs.size(0))\n",
    "            ce_loss_meter.update(ce_loss.item(), inputs.size(0))\n",
    "            triplet_loss_meter.update(trip_loss.item() if trip_loss != 0 else 0, inputs.size(0))\n",
    "            \n",
    "            probs = F.softmax(logits.float(), dim=1).cpu().numpy()\n",
    "            PROBS.append(probs)\n",
    "            TARGETS.append(targets.cpu().numpy())\n",
    "            EMBEDDINGS.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    PROBS = np.concatenate(PROBS)\n",
    "    TARGETS = np.concatenate(TARGETS)\n",
    "    EMBEDDINGS = np.concatenate(EMBEDDINGS)\n",
    "\n",
    "    try:\n",
    "        val_auc = roc_auc_score(TARGETS, PROBS, multi_class='ovr')\n",
    "    except ValueError as e:\n",
    "        print(f\"Val AUC failed: {e}, Unique targets: {np.unique(TARGETS)}, Probs shape: {PROBS.shape}\")\n",
    "        val_auc = 0.0\n",
    "\n",
    "    recall_at_k = compute_recall_at_k(EMBEDDINGS, TARGETS, k=1)\n",
    "    print(f\"Validation Recall@1: {recall_at_k:.5f}\")\n",
    "\n",
    "    return val_loss_meter.avg, val_auc, triplet_loss_meter.avg, ce_loss_meter.avg, recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold, df, root_dir, transforms_train, transforms_val, num_workers, n_epochs, device, batch_size, lr, run_dir, backbone_name):\n",
    "    train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
    "    val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = TripletDataset(root_dir, train_df,'train', transform=transforms_train)\n",
    "    val_ds = TripletDataset(root_dir, val_df,'train', transform=transforms_val)\n",
    "    \n",
    "    # Sampler\n",
    "    train_sampler = get_sampler(train_ds)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False, \n",
    "        pin_memory=True, \n",
    "        sampler=train_sampler\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False, \n",
    "        pin_memory=True, \n",
    "    )\n",
    "\n",
    "    # Model, optimizer, criterion\n",
    "    model = EmbeddingModel(num_classes,backbone_name, 512).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Classification loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Distance function for triplet loss\n",
    "    distance = distances.CosineSimilarity()\n",
    "    \n",
    "    # Batch hard miner for triplet loss\n",
    "    miner = miners.BatchEasyHardMiner(\n",
    "        pos_strategy=\"easy\",  # hardest positive\n",
    "        neg_strategy=\"semihard\",  # hardest negative\n",
    "        distance=distance\n",
    "    )\n",
    "    \n",
    "    # Triplet loss with margin\n",
    "    triplet_loss = losses.TripletMarginLoss(\n",
    "        margin=1,\n",
    "        distance=distance,\n",
    "        reducer=reducers.AvgNonZeroReducer()\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler_cosine = CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
    "    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=5, total_epoch=5, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    # History tracking\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_auc': [], 'val_auc': [],\n",
    "        'triplet_loss': [], 'ce_loss': [],\n",
    "        'learning_rates': [],\n",
    "        'best_val_auc': 0, 'best_val_auc_epoch': 0,\n",
    "        'best_val_loss': float('inf'), 'best_val_loss_epoch': 0\n",
    "    }\n",
    "\n",
    "\n",
    "    print(f\"Fold {fold}: =========================================\")\n",
    "    \n",
    "    early_stopping_active = False\n",
    "    # es = EarlyStopper(min_delta=1e-3, patience=2)\n",
    "    best_model_state_dict = None\n",
    "    best_model_filename = None\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        print(f\"\\nEP {epoch}/{n_epochs} (LR: {current_lr:.6f}):\")\n",
    "        train_loss, train_auc, train_triplet_loss, train_ce_loss = train_epoch(model, train_loader, optimizer, criterion, triplet_loss, miner, device, loss_type=\"CE_trip\")\n",
    "        val_loss, val_auc, val_triplet_loss, val_ce_loss, recall_at_k = val_epoch(model, val_loader, criterion, triplet_loss, miner, device, loss_type='CE_trip')\n",
    "        \n",
    "        print(f\"Train AUC: {train_auc:.4f}, CE Loss: {train_ce_loss:.4f}, Triplet Loss: {train_triplet_loss:.6f}\")\n",
    "        print(f\"Val AUC: {val_auc:.4f}, CE Loss: {val_ce_loss:.4f}, Triplet Loss: {val_triplet_loss:.6f}\")\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_auc'].append(train_auc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        history['triplet_loss'].append(train_triplet_loss)\n",
    "        history['ce_loss'].append(train_ce_loss)\n",
    "        \n",
    "        if val_auc > history['best_val_auc']:\n",
    "            history['best_val_auc'] = val_auc\n",
    "            history['best_val_auc_epoch'] = epoch\n",
    "            best_model_state_dict = model.state_dict()\n",
    "            # checkpoint_path = os.path.join(run_dir, f\"fold{fold}_best_auc{val_auc:.4f}_ep{epoch}.pth\")\n",
    "            # torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"New best AUC: {val_auc:.4f} at epoch {epoch}.\")\n",
    "        \n",
    "        if val_loss < history['best_val_loss']:\n",
    "            history['best_val_loss'] = val_loss\n",
    "            history['best_val_loss_epoch'] = epoch\n",
    "        \n",
    "        # Step the scheduler (use warmup scheduler during warmup, then cosine)\n",
    "        if epoch <= scheduler_warmup.total_epoch:\n",
    "            scheduler_warmup.step()\n",
    "        else:\n",
    "            scheduler_cosine.step()\n",
    "            \n",
    "        if epoch == scheduler_warmup.total_epoch:\n",
    "            early_stopping_active = True\n",
    "            # Reset early stopper to forget the potentially misleading high scores during warmup\n",
    "            es_ce = EarlyStopper(min_delta=1e-5, patience=10)\n",
    "            es_trip = EarlyStopper(min_delta=1e-5, patience=10, use_loss=True)\n",
    "            print(\"Warmup complete. Early stopping now active.\")\n",
    "        \n",
    "        # Only check early stopping if it's active\n",
    "        if early_stopping_active:\n",
    "            if es_ce.early_stop(val_auc) and es_trip.early_stop(train_triplet_loss):\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    # Save the final model and best model\n",
    "    if best_model_state_dict is not None:\n",
    "        best_model_filename = os.path.join(run_dir, f\"fold{fold}_best_auc{history['best_val_auc']:.4f}_ep{history['best_val_auc_epoch']}.pth\")\n",
    "        torch.save(best_model_state_dict, best_model_filename)\n",
    "        print(f\"Best model saved: {best_model_filename}\")\n",
    "\n",
    "    final_checkpoint_path = os.path.join(run_dir, f\"fold{fold}_final.pth\")\n",
    "    torch.save(model.state_dict(), final_checkpoint_path)\n",
    "    print(f\"Final model saved: {final_checkpoint_path}\")\n",
    "\n",
    "    # Plot history\n",
    "    plot_fold_history(fold, history, run_dir)\n",
    "    \n",
    "    # Compute OOF predictions after training\n",
    "    best_model_path = os.path.join(run_dir, f\"fold{fold}_best_auc{history['best_val_auc']:.4f}_ep{history['best_val_auc_epoch']}.pth\")\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.eval()\n",
    "    oof_preds = []\n",
    "    oof_targets = []\n",
    "    oof_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for img, label in val_loader:\n",
    "            inputs = img.to(device)\n",
    "            targets = label.to(device)\n",
    "            logits, embeddings = model(inputs)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            oof_preds.append(probs)\n",
    "            oof_targets.append(targets.cpu().numpy())\n",
    "            oof_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    oof_preds = np.concatenate(oof_preds)\n",
    "    oof_targets = np.concatenate(oof_targets)\n",
    "    oof_embeddings = np.concatenate(oof_embeddings)\n",
    "    \n",
    "    # Save embeddings for later visualization or analysis\n",
    "    embeddings_path = os.path.join(run_dir, f'fold_{fold}_embeddings.npy')\n",
    "    np.save(embeddings_path, oof_embeddings)\n",
    "    \n",
    "    oof_names = val_df['path'].values\n",
    "    oof_folds = np.full(len(oof_targets), fold)\n",
    "    return oof_preds, oof_targets, oof_names, oof_folds, oof_embeddings\n",
    "\n",
    "# Visualization function for embeddings using t-SNE\n",
    "def visualize_embeddings(embeddings, labels, title=\"t-SNE Visualization of Embeddings\", run_dir=None):\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for class_idx in np.unique(labels):\n",
    "        plt.scatter(\n",
    "            embeddings_2d[labels == class_idx, 0],\n",
    "            embeddings_2d[labels == class_idx, 1],\n",
    "            label=f'Class {class_idx}'\n",
    "        )\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(run_dir, f\"{title.replace(' ', '_')}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE= 600\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 16\n",
    "n_epochs = 100\n",
    "num_workers = 0\n",
    "print(f\"Num workers = {num_workers}\")\n",
    "folds=[0,1,2,3,4]\n",
    "lr = 3e-4\n",
    "backbone_name='resnet50'\n",
    "\n",
    "checkpoint_dir = '../checkpoints'\n",
    "run_num = 1\n",
    "while True:\n",
    "    run_dir = os.path.join(checkpoint_dir, f\"run{run_num}\")\n",
    "    if not os.path.exists(run_dir):\n",
    "        os.makedirs(run_dir)\n",
    "        break\n",
    "    run_num += 1\n",
    "print(f\"Checkpoints will be saved in: {run_dir}\")\n",
    "\n",
    "oof_preds_all = []\n",
    "oof_targets_all = []\n",
    "oof_names_all = []\n",
    "oof_folds_all = []\n",
    "oof_embeddings_all = []\n",
    "\n",
    "transforms_train, transforms_val = get_transforms(IMG_SIZE)\n",
    "\n",
    "for fold in folds:\n",
    "    oof_preds, oof_targets, oof_names, oof_folds, oof_embeddings = run(\n",
    "        fold, df, root_dir, transforms_train, transforms_val, num_workers, n_epochs, device, batch_size, lr, run_dir, backbone_name\n",
    "    )\n",
    "    oof_preds_all.append(oof_preds)\n",
    "    oof_targets_all.append(oof_targets)\n",
    "    oof_names_all.append(oof_names)\n",
    "    oof_folds_all.append(oof_folds)\n",
    "    \n",
    "    # Visualize embeddings for this fold\n",
    "    visualize_embeddings(oof_embeddings, oof_targets, f\"Fold {fold} Embeddings\", run_dir)\n",
    "\n",
    "# Concatenate OOF data\n",
    "oof_preds_all = np.concatenate(oof_preds_all)\n",
    "oof_targets_all = np.concatenate(oof_targets_all)\n",
    "oof_names_all = np.concatenate(oof_names_all)\n",
    "oof_folds_all = np.concatenate(oof_folds_all)\n",
    "# oof_embeddings_all = np.concatenate(oof_embeddings_all)\n",
    "\n",
    "# Visualize all embeddings together\n",
    "# visualize_embeddings(oof_embeddings_all, oof_targets_all, \"All Folds Embeddings\", run_dir)\n",
    "\n",
    "# Compute overall OOF AUC\n",
    "auc = roc_auc_score(oof_targets_all, oof_preds_all, multi_class='ovr')\n",
    "print(f'Overall OOF AUC = {auc:.3f}')\n",
    "\n",
    "# Save OOF to CSV with class probabilities\n",
    "# Create a dictionary for the DataFrame\n",
    "df_oof_dict = {\n",
    "    'image_name': oof_names_all,\n",
    "    'target': oof_targets_all,\n",
    "    'fold': oof_folds_all\n",
    "}\n",
    "\n",
    "# Add probability columns for each class\n",
    "for i, class_name in enumerate(class_list):\n",
    "    df_oof_dict[f'prob_{class_name}'] = oof_preds_all[:, i]\n",
    "\n",
    "# Create DataFrame\n",
    "df_oof = pd.DataFrame(df_oof_dict)\n",
    "\n",
    "# Save to run_dir\n",
    "oof_path = os.path.join(run_dir, 'oof_triplet.csv')\n",
    "df_oof.to_csv(oof_path, index=False)\n",
    "print(f\"OOF saved to: {oof_path}\")\n",
    "print(df_oof.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
