{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# Import PyTorch Metric Learning\n",
    "from pytorch_metric_learning import losses, miners, reducers, distances\n",
    "from pytorch_metric_learning.utils import accuracy_calculator\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2 #np.array -> torch.tensor (B, 3, H, W)\n",
    "import timm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce GTX 1050\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "root_dir = '../datasets/'\n",
    "csv_train_file = 'train_data_with_folds.csv'\n",
    "class_list = ['normal', 'preplus', 'plus']\n",
    "label_dict = {cls: i for i, cls in enumerate(class_list)}\n",
    "\n",
    "df = pd.read_csv(os.path.join(root_dir, csv_train_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, mode, transform=None):\n",
    "        self.root = root_dir\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.df = df\n",
    "        self.img_path_list = df['path'].tolist()\n",
    "        \n",
    "        if 'label' in df.columns:\n",
    "            self.labels = df['label'].tolist()\n",
    "        else:\n",
    "            self.labels = None   \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root, self.img_path_list[idx])\n",
    "        \n",
    "        # Try OpenCV first (faster)\n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(\"OpenCV couldn't read the image\")\n",
    "            # Convert BGR (OpenCV default) to RGB\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        # Fallback to PIL if OpenCV fails\n",
    "        except:\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image = np.array(image)\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Failed to load image {image_path} with both OpenCV and PIL: {str(e)}\")\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform is not None:\n",
    "            # Ensure image is in correct format for transforms\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return image\n",
    "        else:\n",
    "            label = self.labels[idx]\n",
    "            return image, torch.tensor(label).long()\n",
    "\n",
    "def get_transforms(image_size):\n",
    "    transforms_train = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.ImageCompression(quality_lower=80, quality_upper=100, p=0.25),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.5),\n",
    "        # A.Flip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.CoarseDropout(num_holes_range=(1,1), hole_height_range=(8, 32), hole_width_range=(8, 32), p=0.25),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    transforms_val = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    return transforms_train, transforms_val\n",
    "\n",
    "class UnNormalize(object):\n",
    "  def __init__(self, mean, std):\n",
    "    self.mean = mean\n",
    "    self.std = std\n",
    "\n",
    "  def __call__(self, tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      tensor (Tensor): Tensor image of size (C, H, W) to be normalized'\n",
    "    Returns:\n",
    "      Tensor: Normalized image\n",
    "    \"\"\"\n",
    "    for t, m, s in zip(tensor, self.mean, self.std):\n",
    "      t.mul_(s).add_(m)\n",
    "      #The normalize code -> t.sub_(m).div_(s)\n",
    "    return tensor\n",
    "\n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "\n",
    "def get_sampler(dataset): # WeightedRandomSampler\n",
    "    labels = [dataset.dataset[idx][1] for idx in range(len(dataset))]\n",
    "    class_counts = np.bincount(labels, minlength=num_classes)\n",
    "    class_weights = 1.0 / (class_counts + 1e-6)\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    return sampler\n",
    "\n",
    "# Modified model for embedding output\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size=512):\n",
    "        super().__init__()\n",
    "        self.n_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # Use EfficientNet as the backbone\n",
    "        self.backbone = timm.create_model(\n",
    "            'resnet101',\n",
    "            pretrained=True,\n",
    "            features_only=True,  # Remove classifier head\n",
    "        )\n",
    "        \n",
    "        # Get the feature dimension from backbone\n",
    "        self.in_features = self.backbone.num_features\n",
    "        \n",
    "        # Add embedding layer\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(self.in_features, self.embedding_size),\n",
    "            nn.BatchNorm1d(self.embedding_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Classifier layer (for regular classification task)\n",
    "        self.classifier = nn.Linear(self.embedding_size, self.n_classes)\n",
    "\n",
    "    def forward(self, x, return_embeddings=False):\n",
    "        features = self.backbone(x)\n",
    "        embeddings = self.embedding(features)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return embeddings\n",
    "        \n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits, embeddings\n",
    "\n",
    "# Plotting function\n",
    "def plot_fold_history(fold, history):\n",
    "    actual_epochs = len(history['train_auc'])\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot auc\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, actual_epochs + 1), history['train_auc'], '-o', label='Train AUC', color='skyblue')\n",
    "    plt.plot(range(1, actual_epochs + 1), history['val_auc'], '-o', label='Val AUC', color='lightcoral')\n",
    "    plt.scatter(history['best_val_auc_epoch'], history['best_val_auc'], s=200, color='lightcoral')\n",
    "    plt.text(history['best_val_auc_epoch'], history['best_val_auc'], f'max {history[\"best_val_auc\"]:.4f}', size=12)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('auc')\n",
    "    plt.title(f'Fold {fold + 1} Auc')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, actual_epochs + 1), history['train_loss'], '-o', label='Train Loss', color='skyblue')\n",
    "    plt.plot(range(1, actual_epochs + 1), history['val_loss'], '-o', label='Val Loss', color='lightcoral')\n",
    "    plt.plot(range(1, actual_epochs + 1), history['triplet_loss'], '-o', label='Triplet Loss', color='green')\n",
    "    plt.scatter(history['best_val_loss_epoch'], history['best_val_loss'], s=200, color='lightcoral')\n",
    "    plt.text(history['best_val_loss_epoch'], history['best_val_loss'], f'min {history[\"best_val_loss\"]:.4f}', size=12)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {fold + 1} Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, min_delta=0, patience=1):\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.max_val_auc = -float('inf')\n",
    "        self.count = 0\n",
    "        \n",
    "    def early_stop(self, val_auc):\n",
    "        if self.max_val_auc < val_auc:\n",
    "            self.max_val_auc = val_auc\n",
    "            self.count = 0\n",
    "        elif self.max_val_auc > val_auc + self.min_delta:\n",
    "            self.count += 1\n",
    "            if self.count >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "# Custom GradualWarmupSchedulerV2\n",
    "class GradualWarmupSchedulerV2:\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.multiplier = multiplier\n",
    "        self.total_epoch = total_epoch\n",
    "        self.after_scheduler = after_scheduler\n",
    "        self.finished = False\n",
    "        self.last_epoch = -1\n",
    "        self.base_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "    \n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            self.last_epoch += 1\n",
    "        else:\n",
    "            self.last_epoch = epoch\n",
    "        \n",
    "        if self.last_epoch <= self.total_epoch:\n",
    "            # Warmup phase\n",
    "            for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "                param_group['lr'] = lr\n",
    "        elif self.after_scheduler:\n",
    "            # Transition to after_scheduler\n",
    "            if not self.finished:\n",
    "                self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                self.finished = True\n",
    "            self.after_scheduler.step(self.last_epoch - self.total_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "              ReLU-3         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-4         [-1, 64, 128, 128]               0\n",
      "            Conv2d-5         [-1, 64, 128, 128]           4,096\n",
      "       BatchNorm2d-6         [-1, 64, 128, 128]             128\n",
      "              ReLU-7         [-1, 64, 128, 128]               0\n",
      "            Conv2d-8         [-1, 64, 128, 128]          36,864\n",
      "       BatchNorm2d-9         [-1, 64, 128, 128]             128\n",
      "         Identity-10         [-1, 64, 128, 128]               0\n",
      "             ReLU-11         [-1, 64, 128, 128]               0\n",
      "         Identity-12         [-1, 64, 128, 128]               0\n",
      "           Conv2d-13        [-1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-14        [-1, 256, 128, 128]             512\n",
      "           Conv2d-15        [-1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-16        [-1, 256, 128, 128]             512\n",
      "             ReLU-17        [-1, 256, 128, 128]               0\n",
      "       Bottleneck-18        [-1, 256, 128, 128]               0\n",
      "           Conv2d-19         [-1, 64, 128, 128]          16,384\n",
      "      BatchNorm2d-20         [-1, 64, 128, 128]             128\n",
      "             ReLU-21         [-1, 64, 128, 128]               0\n",
      "           Conv2d-22         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-23         [-1, 64, 128, 128]             128\n",
      "         Identity-24         [-1, 64, 128, 128]               0\n",
      "             ReLU-25         [-1, 64, 128, 128]               0\n",
      "         Identity-26         [-1, 64, 128, 128]               0\n",
      "           Conv2d-27        [-1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-28        [-1, 256, 128, 128]             512\n",
      "             ReLU-29        [-1, 256, 128, 128]               0\n",
      "       Bottleneck-30        [-1, 256, 128, 128]               0\n",
      "           Conv2d-31         [-1, 64, 128, 128]          16,384\n",
      "      BatchNorm2d-32         [-1, 64, 128, 128]             128\n",
      "             ReLU-33         [-1, 64, 128, 128]               0\n",
      "           Conv2d-34         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-35         [-1, 64, 128, 128]             128\n",
      "         Identity-36         [-1, 64, 128, 128]               0\n",
      "             ReLU-37         [-1, 64, 128, 128]               0\n",
      "         Identity-38         [-1, 64, 128, 128]               0\n",
      "           Conv2d-39        [-1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-40        [-1, 256, 128, 128]             512\n",
      "             ReLU-41        [-1, 256, 128, 128]               0\n",
      "       Bottleneck-42        [-1, 256, 128, 128]               0\n",
      "           Conv2d-43        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-44        [-1, 128, 128, 128]             256\n",
      "             ReLU-45        [-1, 128, 128, 128]               0\n",
      "           Conv2d-46          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-47          [-1, 128, 64, 64]             256\n",
      "         Identity-48          [-1, 128, 64, 64]               0\n",
      "             ReLU-49          [-1, 128, 64, 64]               0\n",
      "         Identity-50          [-1, 128, 64, 64]               0\n",
      "           Conv2d-51          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-52          [-1, 512, 64, 64]           1,024\n",
      "           Conv2d-53          [-1, 512, 64, 64]         131,072\n",
      "      BatchNorm2d-54          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-55          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-56          [-1, 512, 64, 64]               0\n",
      "           Conv2d-57          [-1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-58          [-1, 128, 64, 64]             256\n",
      "             ReLU-59          [-1, 128, 64, 64]               0\n",
      "           Conv2d-60          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 64, 64]             256\n",
      "         Identity-62          [-1, 128, 64, 64]               0\n",
      "             ReLU-63          [-1, 128, 64, 64]               0\n",
      "         Identity-64          [-1, 128, 64, 64]               0\n",
      "           Conv2d-65          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-67          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-68          [-1, 512, 64, 64]               0\n",
      "           Conv2d-69          [-1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 64, 64]             256\n",
      "             ReLU-71          [-1, 128, 64, 64]               0\n",
      "           Conv2d-72          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 64, 64]             256\n",
      "         Identity-74          [-1, 128, 64, 64]               0\n",
      "             ReLU-75          [-1, 128, 64, 64]               0\n",
      "         Identity-76          [-1, 128, 64, 64]               0\n",
      "           Conv2d-77          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-78          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-79          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-80          [-1, 512, 64, 64]               0\n",
      "           Conv2d-81          [-1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-82          [-1, 128, 64, 64]             256\n",
      "             ReLU-83          [-1, 128, 64, 64]               0\n",
      "           Conv2d-84          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-85          [-1, 128, 64, 64]             256\n",
      "         Identity-86          [-1, 128, 64, 64]               0\n",
      "             ReLU-87          [-1, 128, 64, 64]               0\n",
      "         Identity-88          [-1, 128, 64, 64]               0\n",
      "           Conv2d-89          [-1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-90          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-91          [-1, 512, 64, 64]               0\n",
      "       Bottleneck-92          [-1, 512, 64, 64]               0\n",
      "           Conv2d-93          [-1, 256, 64, 64]         131,072\n",
      "      BatchNorm2d-94          [-1, 256, 64, 64]             512\n",
      "             ReLU-95          [-1, 256, 64, 64]               0\n",
      "           Conv2d-96          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 32, 32]             512\n",
      "         Identity-98          [-1, 256, 32, 32]               0\n",
      "             ReLU-99          [-1, 256, 32, 32]               0\n",
      "        Identity-100          [-1, 256, 32, 32]               0\n",
      "          Conv2d-101         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-102         [-1, 1024, 32, 32]           2,048\n",
      "          Conv2d-103         [-1, 1024, 32, 32]         524,288\n",
      "     BatchNorm2d-104         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-105         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-106         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-107          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-108          [-1, 256, 32, 32]             512\n",
      "            ReLU-109          [-1, 256, 32, 32]               0\n",
      "          Conv2d-110          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-111          [-1, 256, 32, 32]             512\n",
      "        Identity-112          [-1, 256, 32, 32]               0\n",
      "            ReLU-113          [-1, 256, 32, 32]               0\n",
      "        Identity-114          [-1, 256, 32, 32]               0\n",
      "          Conv2d-115         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-116         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-117         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-118         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-119          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-120          [-1, 256, 32, 32]             512\n",
      "            ReLU-121          [-1, 256, 32, 32]               0\n",
      "          Conv2d-122          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-123          [-1, 256, 32, 32]             512\n",
      "        Identity-124          [-1, 256, 32, 32]               0\n",
      "            ReLU-125          [-1, 256, 32, 32]               0\n",
      "        Identity-126          [-1, 256, 32, 32]               0\n",
      "          Conv2d-127         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-129         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-130         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-131          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 32, 32]             512\n",
      "            ReLU-133          [-1, 256, 32, 32]               0\n",
      "          Conv2d-134          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 32, 32]             512\n",
      "        Identity-136          [-1, 256, 32, 32]               0\n",
      "            ReLU-137          [-1, 256, 32, 32]               0\n",
      "        Identity-138          [-1, 256, 32, 32]               0\n",
      "          Conv2d-139         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-140         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-141         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-142         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-143          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-144          [-1, 256, 32, 32]             512\n",
      "            ReLU-145          [-1, 256, 32, 32]               0\n",
      "          Conv2d-146          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-147          [-1, 256, 32, 32]             512\n",
      "        Identity-148          [-1, 256, 32, 32]               0\n",
      "            ReLU-149          [-1, 256, 32, 32]               0\n",
      "        Identity-150          [-1, 256, 32, 32]               0\n",
      "          Conv2d-151         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-152         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-153         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-154         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-155          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-156          [-1, 256, 32, 32]             512\n",
      "            ReLU-157          [-1, 256, 32, 32]               0\n",
      "          Conv2d-158          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-159          [-1, 256, 32, 32]             512\n",
      "        Identity-160          [-1, 256, 32, 32]               0\n",
      "            ReLU-161          [-1, 256, 32, 32]               0\n",
      "        Identity-162          [-1, 256, 32, 32]               0\n",
      "          Conv2d-163         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-164         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-165         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-166         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-167          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-168          [-1, 256, 32, 32]             512\n",
      "            ReLU-169          [-1, 256, 32, 32]               0\n",
      "          Conv2d-170          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-171          [-1, 256, 32, 32]             512\n",
      "        Identity-172          [-1, 256, 32, 32]               0\n",
      "            ReLU-173          [-1, 256, 32, 32]               0\n",
      "        Identity-174          [-1, 256, 32, 32]               0\n",
      "          Conv2d-175         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-176         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-177         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-178         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-179          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-180          [-1, 256, 32, 32]             512\n",
      "            ReLU-181          [-1, 256, 32, 32]               0\n",
      "          Conv2d-182          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-183          [-1, 256, 32, 32]             512\n",
      "        Identity-184          [-1, 256, 32, 32]               0\n",
      "            ReLU-185          [-1, 256, 32, 32]               0\n",
      "        Identity-186          [-1, 256, 32, 32]               0\n",
      "          Conv2d-187         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-189         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-190         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-191          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 32, 32]             512\n",
      "            ReLU-193          [-1, 256, 32, 32]               0\n",
      "          Conv2d-194          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 32, 32]             512\n",
      "        Identity-196          [-1, 256, 32, 32]               0\n",
      "            ReLU-197          [-1, 256, 32, 32]               0\n",
      "        Identity-198          [-1, 256, 32, 32]               0\n",
      "          Conv2d-199         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-200         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-201         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-202         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-203          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-204          [-1, 256, 32, 32]             512\n",
      "            ReLU-205          [-1, 256, 32, 32]               0\n",
      "          Conv2d-206          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-207          [-1, 256, 32, 32]             512\n",
      "        Identity-208          [-1, 256, 32, 32]               0\n",
      "            ReLU-209          [-1, 256, 32, 32]               0\n",
      "        Identity-210          [-1, 256, 32, 32]               0\n",
      "          Conv2d-211         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-212         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-213         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-214         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-215          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-216          [-1, 256, 32, 32]             512\n",
      "            ReLU-217          [-1, 256, 32, 32]               0\n",
      "          Conv2d-218          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-219          [-1, 256, 32, 32]             512\n",
      "        Identity-220          [-1, 256, 32, 32]               0\n",
      "            ReLU-221          [-1, 256, 32, 32]               0\n",
      "        Identity-222          [-1, 256, 32, 32]               0\n",
      "          Conv2d-223         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-224         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-225         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-226         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-227          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-228          [-1, 256, 32, 32]             512\n",
      "            ReLU-229          [-1, 256, 32, 32]               0\n",
      "          Conv2d-230          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-231          [-1, 256, 32, 32]             512\n",
      "        Identity-232          [-1, 256, 32, 32]               0\n",
      "            ReLU-233          [-1, 256, 32, 32]               0\n",
      "        Identity-234          [-1, 256, 32, 32]               0\n",
      "          Conv2d-235         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-236         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-237         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-238         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-239          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-240          [-1, 256, 32, 32]             512\n",
      "            ReLU-241          [-1, 256, 32, 32]               0\n",
      "          Conv2d-242          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-243          [-1, 256, 32, 32]             512\n",
      "        Identity-244          [-1, 256, 32, 32]               0\n",
      "            ReLU-245          [-1, 256, 32, 32]               0\n",
      "        Identity-246          [-1, 256, 32, 32]               0\n",
      "          Conv2d-247         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-249         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-250         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-251          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 32, 32]             512\n",
      "            ReLU-253          [-1, 256, 32, 32]               0\n",
      "          Conv2d-254          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 32, 32]             512\n",
      "        Identity-256          [-1, 256, 32, 32]               0\n",
      "            ReLU-257          [-1, 256, 32, 32]               0\n",
      "        Identity-258          [-1, 256, 32, 32]               0\n",
      "          Conv2d-259         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-260         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-261         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-262         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-263          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-264          [-1, 256, 32, 32]             512\n",
      "            ReLU-265          [-1, 256, 32, 32]               0\n",
      "          Conv2d-266          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-267          [-1, 256, 32, 32]             512\n",
      "        Identity-268          [-1, 256, 32, 32]               0\n",
      "            ReLU-269          [-1, 256, 32, 32]               0\n",
      "        Identity-270          [-1, 256, 32, 32]               0\n",
      "          Conv2d-271         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-272         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-273         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-274         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-275          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-276          [-1, 256, 32, 32]             512\n",
      "            ReLU-277          [-1, 256, 32, 32]               0\n",
      "          Conv2d-278          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-279          [-1, 256, 32, 32]             512\n",
      "        Identity-280          [-1, 256, 32, 32]               0\n",
      "            ReLU-281          [-1, 256, 32, 32]               0\n",
      "        Identity-282          [-1, 256, 32, 32]               0\n",
      "          Conv2d-283         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-284         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-285         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-286         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-287          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-288          [-1, 256, 32, 32]             512\n",
      "            ReLU-289          [-1, 256, 32, 32]               0\n",
      "          Conv2d-290          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-291          [-1, 256, 32, 32]             512\n",
      "        Identity-292          [-1, 256, 32, 32]               0\n",
      "            ReLU-293          [-1, 256, 32, 32]               0\n",
      "        Identity-294          [-1, 256, 32, 32]               0\n",
      "          Conv2d-295         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-296         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-297         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-298         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-299          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-300          [-1, 256, 32, 32]             512\n",
      "            ReLU-301          [-1, 256, 32, 32]               0\n",
      "          Conv2d-302          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-303          [-1, 256, 32, 32]             512\n",
      "        Identity-304          [-1, 256, 32, 32]               0\n",
      "            ReLU-305          [-1, 256, 32, 32]               0\n",
      "        Identity-306          [-1, 256, 32, 32]               0\n",
      "          Conv2d-307         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-309         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-310         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-311          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-312          [-1, 256, 32, 32]             512\n",
      "            ReLU-313          [-1, 256, 32, 32]               0\n",
      "          Conv2d-314          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-315          [-1, 256, 32, 32]             512\n",
      "        Identity-316          [-1, 256, 32, 32]               0\n",
      "            ReLU-317          [-1, 256, 32, 32]               0\n",
      "        Identity-318          [-1, 256, 32, 32]               0\n",
      "          Conv2d-319         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-320         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-321         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-322         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-323          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-324          [-1, 256, 32, 32]             512\n",
      "            ReLU-325          [-1, 256, 32, 32]               0\n",
      "          Conv2d-326          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-327          [-1, 256, 32, 32]             512\n",
      "        Identity-328          [-1, 256, 32, 32]               0\n",
      "            ReLU-329          [-1, 256, 32, 32]               0\n",
      "        Identity-330          [-1, 256, 32, 32]               0\n",
      "          Conv2d-331         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-332         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-333         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-334         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-335          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-336          [-1, 256, 32, 32]             512\n",
      "            ReLU-337          [-1, 256, 32, 32]               0\n",
      "          Conv2d-338          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-339          [-1, 256, 32, 32]             512\n",
      "        Identity-340          [-1, 256, 32, 32]               0\n",
      "            ReLU-341          [-1, 256, 32, 32]               0\n",
      "        Identity-342          [-1, 256, 32, 32]               0\n",
      "          Conv2d-343         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-344         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-345         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-346         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-347          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-348          [-1, 256, 32, 32]             512\n",
      "            ReLU-349          [-1, 256, 32, 32]               0\n",
      "          Conv2d-350          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-351          [-1, 256, 32, 32]             512\n",
      "        Identity-352          [-1, 256, 32, 32]               0\n",
      "            ReLU-353          [-1, 256, 32, 32]               0\n",
      "        Identity-354          [-1, 256, 32, 32]               0\n",
      "          Conv2d-355         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-356         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-357         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-358         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-359          [-1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-360          [-1, 256, 32, 32]             512\n",
      "            ReLU-361          [-1, 256, 32, 32]               0\n",
      "          Conv2d-362          [-1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-363          [-1, 256, 32, 32]             512\n",
      "        Identity-364          [-1, 256, 32, 32]               0\n",
      "            ReLU-365          [-1, 256, 32, 32]               0\n",
      "        Identity-366          [-1, 256, 32, 32]               0\n",
      "          Conv2d-367         [-1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-368         [-1, 1024, 32, 32]           2,048\n",
      "            ReLU-369         [-1, 1024, 32, 32]               0\n",
      "      Bottleneck-370         [-1, 1024, 32, 32]               0\n",
      "          Conv2d-371          [-1, 512, 32, 32]         524,288\n",
      "     BatchNorm2d-372          [-1, 512, 32, 32]           1,024\n",
      "            ReLU-373          [-1, 512, 32, 32]               0\n",
      "          Conv2d-374          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-375          [-1, 512, 16, 16]           1,024\n",
      "        Identity-376          [-1, 512, 16, 16]               0\n",
      "            ReLU-377          [-1, 512, 16, 16]               0\n",
      "        Identity-378          [-1, 512, 16, 16]               0\n",
      "          Conv2d-379         [-1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-380         [-1, 2048, 16, 16]           4,096\n",
      "          Conv2d-381         [-1, 2048, 16, 16]       2,097,152\n",
      "     BatchNorm2d-382         [-1, 2048, 16, 16]           4,096\n",
      "            ReLU-383         [-1, 2048, 16, 16]               0\n",
      "      Bottleneck-384         [-1, 2048, 16, 16]               0\n",
      "          Conv2d-385          [-1, 512, 16, 16]       1,048,576\n",
      "     BatchNorm2d-386          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-387          [-1, 512, 16, 16]               0\n",
      "          Conv2d-388          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-389          [-1, 512, 16, 16]           1,024\n",
      "        Identity-390          [-1, 512, 16, 16]               0\n",
      "            ReLU-391          [-1, 512, 16, 16]               0\n",
      "        Identity-392          [-1, 512, 16, 16]               0\n",
      "          Conv2d-393         [-1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-394         [-1, 2048, 16, 16]           4,096\n",
      "            ReLU-395         [-1, 2048, 16, 16]               0\n",
      "      Bottleneck-396         [-1, 2048, 16, 16]               0\n",
      "          Conv2d-397          [-1, 512, 16, 16]       1,048,576\n",
      "     BatchNorm2d-398          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-399          [-1, 512, 16, 16]               0\n",
      "          Conv2d-400          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-401          [-1, 512, 16, 16]           1,024\n",
      "        Identity-402          [-1, 512, 16, 16]               0\n",
      "            ReLU-403          [-1, 512, 16, 16]               0\n",
      "        Identity-404          [-1, 512, 16, 16]               0\n",
      "          Conv2d-405         [-1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-406         [-1, 2048, 16, 16]           4,096\n",
      "            ReLU-407         [-1, 2048, 16, 16]               0\n",
      "      Bottleneck-408         [-1, 2048, 16, 16]               0\n",
      "================================================================\n",
      "Total params: 42,500,160\n",
      "Trainable params: 42,500,160\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 2423.00\n",
      "Params size (MB): 162.13\n",
      "Estimated Total Size (MB): 2588.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = timm.create_model(\n",
    "            'resnet101',\n",
    "            pretrained=True,\n",
    "            features_only=True,  # Remove classifier head\n",
    "        ).to(device)\n",
    "summary(model, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, triplet_loss, miner, device):\n",
    "    train_loss_meter = AverageMeter()\n",
    "    triplet_loss_meter = AverageMeter()\n",
    "    ce_loss_meter = AverageMeter()\n",
    "    model.train()\n",
    "    \n",
    "    PROBS = []\n",
    "    TARGETS = []\n",
    "    \n",
    "    for img, label in loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = img.to(device)\n",
    "        targets = label.to(device)\n",
    "        \n",
    "        # Get both logits and embeddings\n",
    "        logits, embeddings = model(inputs)\n",
    "        \n",
    "        # Calculate classification loss\n",
    "        ce_loss = criterion(logits, targets)\n",
    "        \n",
    "        # Calculate triplet loss with hard mining\n",
    "        hard_pairs = miner(embeddings, targets)\n",
    "        trip_loss = triplet_loss(embeddings, targets, hard_pairs)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = ce_loss + trip_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_meter.update(loss.item(), inputs.size(0))\n",
    "        ce_loss_meter.update(ce_loss.item(), inputs.size(0))\n",
    "        triplet_loss_meter.update(trip_loss.item() if trip_loss != 0 else 0, inputs.size(0))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs = F.softmax(logits.float(), dim=1).cpu().numpy()\n",
    "            PROBS.append(probs)\n",
    "            TARGETS.append(targets.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    PROBS = np.concatenate(PROBS)\n",
    "    TARGETS = np.concatenate(TARGETS)\n",
    "\n",
    "    if not np.allclose(PROBS.sum(axis=1), 1.0, atol=1e-5):\n",
    "        print(\"PROBS not summing to 1!\")\n",
    "    if np.any(TARGETS < 0) or np.any(TARGETS >= num_classes):\n",
    "        print(f\"Invalid TARGETS values: {TARGETS}\")\n",
    "\n",
    "    # Compute AUC over entire epoch\n",
    "    try:\n",
    "        train_auc = roc_auc_score(y_true=TARGETS, y_score=PROBS, multi_class='ovr')\n",
    "    except ValueError as e:\n",
    "        print(f\"Sample of PROBS: {PROBS[0]}, sum: {PROBS.sum(axis=1)}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        train_auc = 0.0\n",
    "        \n",
    "    return train_loss_meter.avg, train_auc, triplet_loss_meter.avg, ce_loss_meter.avg\n",
    "\n",
    "# Validation epoch\n",
    "def val_epoch(model, loader, criterion, triplet_loss, miner, device):\n",
    "    model.eval()\n",
    "    val_loss_meter = AverageMeter()\n",
    "    triplet_loss_meter = AverageMeter()\n",
    "    ce_loss_meter = AverageMeter()\n",
    "    \n",
    "    PROBS = []\n",
    "    TARGETS = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in loader:\n",
    "            inputs = img.to(device)\n",
    "            targets = label.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                logits, embeddings = model(inputs)\n",
    "                \n",
    "                # Calculate classification loss\n",
    "                ce_loss = criterion(logits, targets)\n",
    "                \n",
    "                # Calculate triplet loss with hard mining\n",
    "                hard_pairs = miner(embeddings, targets)\n",
    "                trip_loss = triplet_loss(embeddings, targets, hard_pairs)\n",
    "                \n",
    "                # Combine losses\n",
    "                loss = ce_loss + trip_loss\n",
    "            \n",
    "            val_loss_meter.update(loss.item(), inputs.size(0))\n",
    "            ce_loss_meter.update(ce_loss.item(), inputs.size(0))\n",
    "            triplet_loss_meter.update(trip_loss.item() if trip_loss != 0 else 0, inputs.size(0))\n",
    "            \n",
    "            probs = F.softmax(logits.float(), dim=1).cpu().numpy()\n",
    "            PROBS.append(probs)\n",
    "            TARGETS.append(targets.cpu().numpy())\n",
    "    \n",
    "    PROBS = np.concatenate(PROBS)\n",
    "    TARGETS = np.concatenate(TARGETS)\n",
    "    \n",
    "    try:\n",
    "        val_auc = roc_auc_score(TARGETS, PROBS, multi_class='ovr')\n",
    "    except ValueError as e:\n",
    "        print(f\"Val AUC failed: {e}, Unique targets: {np.unique(TARGETS)}, Probs shape: {PROBS.shape}\")\n",
    "        val_auc = 0.0\n",
    "    \n",
    "    return val_loss_meter.avg, val_auc, triplet_loss_meter.avg, ce_loss_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run(fold, df, root_dir, test_df, transforms_train, transforms_val, num_workers, n_epochs):\n",
    "    train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
    "    val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = TWFoodDataset(root_dir, train_df,'train', transform=transforms_train)\n",
    "    val_ds = TWFoodDataset(root_dir, val_df,'train', transform=transforms_val)\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=True, \n",
    "        pin_memory=True, \n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False, \n",
    "        pin_memory=True, \n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    # Model, optimizer, criterion\n",
    "    model = EmbeddingModel(num_classes=num_classes, embedding_size=128).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Classification loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Distance function for triplet loss\n",
    "    distance = distances.CosineSimilarity()\n",
    "    \n",
    "    # Batch hard miner for triplet loss\n",
    "    miner = miners.BatchHardMiner(\n",
    "        pos_strategy=\"hard\",  # hardest positive\n",
    "        neg_strategy=\"hard\",  # hardest negative\n",
    "        distance=distance\n",
    "    )\n",
    "    \n",
    "    # Triplet loss with margin\n",
    "    triplet_loss = losses.TripletMarginLoss(\n",
    "        margin=0.3,\n",
    "        distance=distance,\n",
    "        reducer=reducers.AvgNonZeroReducer()\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler_cosine = CosineAnnealingWarmRestarts(optimizer, T_0=8)\n",
    "    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=2, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    # History tracking\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_auc': [], 'val_auc': [],\n",
    "        'triplet_loss': [], 'ce_loss': [],\n",
    "        'learning_rates': [],\n",
    "        'best_val_auc': 0, 'best_val_auc_epoch': 0,\n",
    "        'best_val_loss': float('inf'), 'best_val_loss_epoch': 0\n",
    "    }\n",
    "\n",
    "    print(f\"Fold {fold + 1}: =========================================\")\n",
    "    \n",
    "    early_stopping_active = False\n",
    "    es = EarlyStopper(min_delta=1e-3, patience=2)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        print(f\"\\nEP {epoch}/{n_epochs} (LR: {current_lr:.6f}):\")\n",
    "        train_loss, train_auc, train_triplet_loss, train_ce_loss = train_epoch(model, train_loader, optimizer, criterion, triplet_loss, miner, device)\n",
    "        val_loss, val_auc, val_triplet_loss, val_ce_loss = val_epoch(model, val_loader, criterion, triplet_loss, miner, device)\n",
    "        \n",
    "        print(f\"Train AUC: {train_auc:.4f}, CE Loss: {train_ce_loss:.4f}, Triplet Loss: {train_triplet_loss:.4f}\")\n",
    "        print(f\"Val AUC: {val_auc:.4f}, CE Loss: {val_ce_loss:.4f}, Triplet Loss: {val_triplet_loss:.4f}\")\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_auc'].append(train_auc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        history['triplet_loss'].append(train_triplet_loss)\n",
    "        history['ce_loss'].append(train_ce_loss)\n",
    "        \n",
    "        if val_auc > history['best_val_auc']:\n",
    "            history['best_val_auc'] = val_auc\n",
    "            history['best_val_auc_epoch'] = epoch\n",
    "            torch.save(model.state_dict(), f'fold_{fold}_best_auc.pth')\n",
    "            print(f\"New best AUC! Model saved.\")\n",
    "        \n",
    "        if val_loss < history['best_val_loss']:\n",
    "            history['best_val_loss'] = val_loss\n",
    "            history['best_val_loss_epoch'] = epoch\n",
    "        \n",
    "        scheduler_cosine.step()\n",
    "            \n",
    "        if epoch == scheduler_warmup.total_epoch:\n",
    "            early_stopping_active = True\n",
    "            # Reset early stopper to forget the potentially misleading high scores during warmup\n",
    "            es = EarlyStopper(min_delta=1e-3, patience=2)\n",
    "            print(\"Warmup complete. Early stopping now active.\")\n",
    "        \n",
    "        # Only check early stopping if it's active\n",
    "        if early_stopping_active:\n",
    "            if es.early_stop(val_auc):\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "    torch.save(model.state_dict(), f'fold_{fold}_final.pth')\n",
    "    plot_fold_history(fold, history)\n",
    "    \n",
    "    # Compute OOF predictions after training\n",
    "    model.load_state_dict(torch.load(f'fold_{fold}_best_auc.pth'))\n",
    "    model.eval()\n",
    "    oof_preds = []\n",
    "    oof_targets = []\n",
    "    oof_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for img, label in val_loader:\n",
    "            inputs = img.to(device)\n",
    "            targets = label.to(device)\n",
    "            logits, embeddings = model(inputs)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            oof_preds.append(probs)\n",
    "            oof_targets.append(targets.cpu().numpy())\n",
    "            oof_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    oof_preds = np.concatenate(oof_preds)\n",
    "    oof_targets = np.concatenate(oof_targets)\n",
    "    oof_embeddings = np.concatenate(oof_embeddings)\n",
    "    \n",
    "    # Save embeddings for later visualization or analysis\n",
    "    np.save(f'fold_{fold}_embeddings.npy', oof_embeddings)\n",
    "    \n",
    "    # Compute test predictions\n",
    "  \n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, prefetch_factor=2)\n",
    "    test_preds = []\n",
    "    test_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for img in test_loader:\n",
    "            inputs = img.to(device)\n",
    "            logits, embeddings = model(inputs)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            test_preds.append(probs)\n",
    "            test_embeddings.append(embeddings.cpu().numpy())\n",
    "    \n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    test_embeddings = np.concatenate(test_embeddings)\n",
    "    \n",
    "    # Save test embeddings\n",
    "    np.save(f'fold_{fold}_test_embeddings.npy', test_embeddings)\n",
    "    \n",
    "    oof_names = val_df['id'].values\n",
    "    oof_folds = np.full(len(oof_targets), fold)\n",
    "    return oof_preds, oof_targets, oof_names, oof_folds, test_preds, oof_embeddings\n",
    "\n",
    "# Visualization function for embeddings using t-SNE\n",
    "def visualize_embeddings(embeddings, labels, title=\"t-SNE Visualization of Embeddings\"):\n",
    "    from sklearn.manifold import TSNE\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for class_idx in np.unique(labels):\n",
    "        plt.scatter(\n",
    "            embeddings_2d[labels == class_idx, 0],\n",
    "            embeddings_2d[labels == class_idx, 1],\n",
    "            label=f'Class {class_idx}'\n",
    "        )\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title.replace(\" \", \"_\")}.png')\n",
    "    plt.show()\n",
    "\n",
    "IMG_SIZE=256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "n_epochs = 15\n",
    "num_workers = os.cpu_count() #check \n",
    "print(f\"Num workers = {num_workers}\")\n",
    "folds=[0,1,2,3,4]\n",
    "lr = 3e-4\n",
    "\n",
    "oof_preds_all = []\n",
    "oof_targets_all = []\n",
    "oof_names_all = []\n",
    "oof_folds_all = []\n",
    "oof_embeddings_all = []\n",
    "\n",
    "transforms_train, transforms_val = get_transforms(IMG_SIZE)\n",
    "\n",
    "for fold in folds:\n",
    "    oof_preds, oof_targets, oof_names, oof_folds, test_preds, oof_embeddings = run(\n",
    "        fold, df, root_dir, transforms_train, transforms_val, num_workers, n_epochs=n_epochs\n",
    "    )\n",
    "    oof_preds_all.append(oof_preds)\n",
    "    oof_targets_all.append(oof_targets)\n",
    "    oof_names_all.append(oof_names)\n",
    "    oof_folds_all.append(oof_folds)\n",
    "    oof_embeddings_all.append(oof_embeddings)\n",
    "    test_preds_all[:, :, fold] = test_preds\n",
    "    \n",
    "    # Visualize embeddings for this fold\n",
    "    visualize_embeddings(oof_embeddings, oof_targets, f\"Fold {fold} Embeddings\")\n",
    "\n",
    "# Concatenate OOF data\n",
    "oof_preds_all = np.concatenate(oof_preds_all)\n",
    "oof_targets_all = np.concatenate(oof_targets_all)\n",
    "oof_names_all = np.concatenate(oof_names_all)\n",
    "oof_folds_all = np.concatenate(oof_folds_all)\n",
    "oof_embeddings_all = np.concatenate(oof_embeddings_all)\n",
    "\n",
    "# Visualize all embeddings together\n",
    "visualize_embeddings(oof_embeddings_all, oof_targets_all, \"All Folds Embeddings\")\n",
    "\n",
    "# Compute overall OOF AUC\n",
    "auc = roc_auc_score(oof_targets_all, oof_preds_all, multi_class='ovr')\n",
    "print(f'Overall OOF AUC = {auc:.3f}')\n",
    "\n",
    "# Save OOF to CSV\n",
    "df_oof = pd.DataFrame({\n",
    "    'image_name': oof_names_all,\n",
    "    'target': oof_targets_all,\n",
    "    'pred': oof_preds_all.argmax(axis=1),\n",
    "    'fold': oof_folds_all\n",
    "})\n",
    "df_oof.to_csv('oof_triplet.csv', index=False)\n",
    "print(\"OOF saved to 'oof_triplet.csv'\")\n",
    "print(df_oof.head())\n",
    "\n",
    "# Average test predictions across 5 folds\n",
    "test_preds_final = test_preds_all.mean(axis=2)\n",
    "\n",
    "# Save test predictions\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'class': np.argmax(test_preds_final, axis=1)\n",
    "})\n",
    "submission_df.to_csv('submission_triplet.csv', index=False)\n",
    "print(\"Submission saved to 'submission_triplet.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
