{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2 #np.array -> torch.tensor (B, 3, H, W)\n",
    "import timm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "root_dir = '/kaggle/input/tw-food-101/tw_food_101/tw_food_101/'\n",
    "csv_train_file = 'tw_food_101_train_with_folds.csv'\n",
    "csv_test_file = 'tw_food_101_test_list.csv'\n",
    "class_list = pd.read_csv(\"/kaggle/input/tw-food-101/tw_food_101_classes.csv\", header=None).iloc[:,1].tolist()\n",
    "label_dict = {cls: i for i, cls in enumerate(class_list)}\n",
    "\n",
    "df = pd.read_csv(os.path.join(root_dir, csv_train_file))\n",
    "test_df = pd.read_csv(os.path.join(root_dir, csv_test_file), header=None, names=['id','path_to_img'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class TWFoodDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, mode, transform=None):\n",
    "        self.root = root_dir\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.df = df\n",
    "        self.img_path_list = df['path_to_img'].tolist()\n",
    "        \n",
    "        if 'class' in df.columns:\n",
    "            self.labels = df['class'].tolist()\n",
    "        else:\n",
    "            self.labels = None   \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root, self.img_path_list[idx])\n",
    "        \n",
    "        # Try OpenCV first (faster)\n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(\"OpenCV couldn't read the image\")\n",
    "            # Convert BGR (OpenCV default) to RGB\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        # Fallback to PIL if OpenCV fails\n",
    "        except:\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image = np.array(image)\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Failed to load image {image_path} with both OpenCV and PIL: {str(e)}\")\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform is not None:\n",
    "            # Ensure image is in correct format for transforms\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return image\n",
    "        else:\n",
    "            label = self.labels[idx]\n",
    "            return image, torch.tensor(label).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(image_size):\n",
    "    transforms_train = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.ImageCompression(quality_lower=80, quality_upper=100, p=0.25),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.5),\n",
    "        A.Flip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(blur_limit=5),\n",
    "            A.MedianBlur(blur_limit=5),\n",
    "            A.GaussianBlur(blur_limit=5),\n",
    "            A.GaussNoise(var_limit=(5.0, 30.0)),\n",
    "        ], p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.CoarseDropout(num_holes_range=(1,1), hole_height_range=(8, 32), hole_width_range=(8, 32), p=0.25),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    transforms_val = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    return transforms_train, transforms_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_sampler(dataset):\n",
    "    labels = [dataset.dataset[idx][2] for idx in dataset.indices]\n",
    "    class_counts = np.bincount(labels, minlength=num_classes)\n",
    "    class_weights = 1.0 / (class_counts + 1e-6)\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = num_classes\n",
    "        self.backbone = timm.create_model(\n",
    "            'tf_efficientnet_b8.ra_in1k',\n",
    "            pretrained=True,\n",
    "            num_classes=self.n_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "def plot_fold_history(fold, history):\n",
    "    actual_epochs = len(history['train_auc'])\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot auc\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, actual_epochs + 1), history['train_auc'], '-o', label='Train AUC', color='skyblue')\n",
    "    plt.plot(range(1, actual_epochs + 1), history['val_auc'], '-o', label='Val AUC', color='lightcoral')\n",
    "    plt.scatter(history['best_val_auc_epoch'], history['best_val_auc'], s=200, color='lightcoral')\n",
    "    plt.text(history['best_val_auc_epoch'], history['best_val_auc'], f'max {history[\"best_val_auc\"]:.4f}', size=12)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('auc')\n",
    "    plt.title(f'Fold {fold + 1} Auc')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, actual_epochs + 1), history['train_loss'], '-o', label='Train Loss', color='skyblue')\n",
    "    plt.plot(range(1, actual_epochs + 1), history['val_loss'], '-o', label='Val Loss', color='lightcoral')\n",
    "    plt.scatter(history['best_val_loss_epoch'], history['best_val_loss'], s=200, color='lightcoral')\n",
    "    plt.text(history['best_val_loss_epoch'], history['best_val_loss'], f'min {history[\"best_val_loss\"]:.4f}', size=12)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Fold {fold + 1} Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, min_delta=0, patience=1):\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.max_val_auc = -float('inf')\n",
    "        self.count = 0\n",
    "        \n",
    "    def early_stop(self, val_auc):\n",
    "        if self.max_val_auc < val_auc:\n",
    "            self.max_val_auc = val_auc\n",
    "            self.count = 0\n",
    "        elif self.max_val_auc > val_auc + self.min_delta:\n",
    "            self.count += 1\n",
    "            if self.count >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "# Custom GradualWarmupSchedulerV2\n",
    "class GradualWarmupSchedulerV2:\n",
    "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.multiplier = multiplier\n",
    "        self.total_epoch = total_epoch\n",
    "        self.after_scheduler = after_scheduler\n",
    "        self.finished = False\n",
    "        self.last_epoch = -1\n",
    "        self.base_lrs = [group['lr'] for group in optimizer.param_groups]\n",
    "    \n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            self.last_epoch += 1\n",
    "        else:\n",
    "            self.last_epoch = epoch\n",
    "        \n",
    "        if self.last_epoch <= self.total_epoch:\n",
    "            # Warmup phase\n",
    "            for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "                param_group['lr'] = lr\n",
    "        elif self.after_scheduler:\n",
    "            # Transition to after_scheduler\n",
    "            if not self.finished:\n",
    "                self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                self.finished = True\n",
    "            self.after_scheduler.step(self.last_epoch - self.total_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.last_epoch > self.total_epoch:\n",
    "            if self.after_scheduler:\n",
    "                if not self.finished:\n",
    "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "                    self.finished = True\n",
    "                return self.after_scheduler.get_lr()\n",
    "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
    "        if self.multiplier == 1.0:\n",
    "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    train_loss_meter = AverageMeter()\n",
    "    model.train()\n",
    "    \n",
    "    PROBS = []\n",
    "    TARGETS = []\n",
    "    \n",
    "    for img, label in loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = img.to(device)\n",
    "        targets = label.to(device)\n",
    "        \n",
    "        with autocast():\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_meter.update(loss.item(), inputs.size(0))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs = F.softmax(logits.float(), dim=1).cpu().numpy()\n",
    "            PROBS.append(probs)\n",
    "            TARGETS.append(targets.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    PROBS = np.concatenate(PROBS)\n",
    "    TARGETS = np.concatenate(TARGETS)\n",
    "\n",
    "    if not np.allclose(PROBS.sum(axis=1), 1.0, atol=1e-5):\n",
    "        print(\"PROBS not summing to 1!\")\n",
    "    if np.any(TARGETS < 0) or np.any(TARGETS >= 101):\n",
    "        print(f\"Invalid TARGETS values: {TARGETS}\")\n",
    "\n",
    "    # Compute AUC over entire epoch\n",
    "    try:\n",
    "        train_auc = roc_auc_score(y_true=TARGETS, y_score=PROBS, multi_class='ovr')\n",
    "    except ValueError as e:\n",
    "        print(f\"Sample of PROBS: {PROBS[0]}, sum: {PROBS.sum(axis=1)}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    return train_loss_meter.avg, train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation epoch\n",
    "def val_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss_meter = AverageMeter()\n",
    "    \n",
    "    PROBS = []\n",
    "    TARGETS = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in loader:\n",
    "            inputs = img.to(device)\n",
    "            targets = label.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, targets)\n",
    "            \n",
    "            val_loss_meter.update(loss.item(), inputs.size(0))\n",
    "            \n",
    "            probs = F.softmax(logits.float(), dim=1).cpu().numpy()\n",
    "            PROBS.append(probs)\n",
    "            TARGETS.append(targets.cpu().numpy())\n",
    "    \n",
    "    PROBS = np.concatenate(PROBS)\n",
    "    TARGETS = np.concatenate(TARGETS)\n",
    "    \n",
    "    try:\n",
    "        val_auc = roc_auc_score(TARGETS, PROBS, multi_class='ovr')\n",
    "    except ValueError as e:\n",
    "        print(f\"Val AUC failed: {e}, Unique targets: {np.unique(TARGETS)}, Probs shape: {PROBS.shape}\")\n",
    "        val_auc = 0.0\n",
    "    \n",
    "    return val_loss_meter.avg, val_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold, df, root_dir, test_df, transforms_train, transforms_val, num_workers, n_epochs):\n",
    "    train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
    "    val_df = df[df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = TWFoodDataset(root_dir, train_df,'train', transform=transforms_train)\n",
    "    val_ds = TWFoodDataset(root_dir, val_df,'train', transform=transforms_val)\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=True, \n",
    "        pin_memory=True, \n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False, \n",
    "        pin_memory=True, \n",
    "        prefetch_factor=2\n",
    "    )\n",
    "\n",
    "    # Model, optimizer, criterion\n",
    "    model = FoodModel(num_classes=num_classes).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler_cosine = CosineAnnealingWarmRestarts(optimizer, T_0=8)\n",
    "    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=2, after_scheduler=scheduler_cosine)\n",
    "\n",
    "    # History tracking\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_auc': [], 'val_auc': [],\n",
    "        'learning_rates': [],\n",
    "        'best_val_auc': 0, 'best_val_auc_epoch': 0,\n",
    "        'best_val_loss': float('inf'), 'best_val_loss_epoch': 0\n",
    "    }\n",
    "\n",
    "    print(f\"Fold {fold + 1}: =========================================\")\n",
    "    \n",
    "    early_stopping_active = False\n",
    "    # es = EarlyStopper(min_delta=1e-3, patience=3)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        print(f\"\\nEP {epoch}/{n_epochs} (LR: {current_lr:.6f}):\")\n",
    "        train_loss, train_auc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_auc = val_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Train AUC: {train_auc:.4f}\")\n",
    "        print(f\"Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_auc'].append(train_auc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        \n",
    "        if val_auc > history['best_val_auc']:\n",
    "            history['best_val_auc'] = val_auc\n",
    "            history['best_val_auc_epoch'] = epoch\n",
    "            torch.save(model.state_dict(), f'fold_{fold}_best_auc.pth')\n",
    "            print(f\"New best AUC! Model saved.\")\n",
    "        \n",
    "        if val_loss < history['best_val_loss']:\n",
    "            history['best_val_loss'] = val_loss\n",
    "            history['best_val_loss_epoch'] = epoch\n",
    "        \n",
    "        scheduler_cosine.step()\n",
    "        # if epoch == 2: scheduler_warmup.step()  # Bug workaround\n",
    "            \n",
    "        if epoch == scheduler_warmup.total_epoch:\n",
    "            early_stopping_active = True\n",
    "            # Reset early stopper to forget the potentially misleading high scores during warmup\n",
    "            es = EarlyStopper(min_delta=1e-3, patience=2)\n",
    "            print(\"Warmup complete. Early stopping now active.\")\n",
    "        \n",
    "        # Only check early stopping if it's active\n",
    "        if early_stopping_active:\n",
    "            if es.early_stop(val_auc):\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "    torch.save(model.state_dict(), f'fold_{fold}_final.pth')\n",
    "    plot_fold_history(fold, history)\n",
    "    \n",
    "    # Compute OOF predictions after training\n",
    "    model.load_state_dict(torch.load(f'fold_{fold}_best_auc.pth'))\n",
    "    model.eval()\n",
    "    oof_preds = []\n",
    "    oof_targets = []\n",
    "    with torch.no_grad():\n",
    "        for img, label in val_loader:\n",
    "            inputs = img.to(device)\n",
    "            targets = label.to(device)\n",
    "            logits = model(inputs)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            oof_preds.append(probs)\n",
    "            oof_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    oof_preds = np.concatenate(oof_preds)\n",
    "    oof_targets = np.concatenate(oof_targets)\n",
    "    \n",
    "    # Compute test predictions\n",
    "    test_ds = TWFoodDataset(root_dir, test_df, 'test', transform=transforms_val)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, prefetch_factor=2)\n",
    "    test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for img in test_loader:\n",
    "            preds = model(img.to(device))\n",
    "            test_preds.append(F.softmax(preds, dim=1).cpu().numpy())\n",
    "    test_preds = np.concatenate(test_preds)\n",
    "    \n",
    "    oof_names = val_df['id'].values\n",
    "    oof_folds = np.full(len(oof_targets), fold)\n",
    "    return oof_preds, oof_targets, oof_names, oof_folds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=256\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 32\n",
    "n_epochs = 15\n",
    "num_workers = os.cpu_count() #check \n",
    "print(f\"Num workers = {num_workers}\")\n",
    "folds=[0,1,2,3,4]\n",
    "lr = 3e-4\n",
    "\n",
    "oof_preds_all = []\n",
    "oof_targets_all = []\n",
    "test_preds_all = np.zeros((len(test_df), num_classes, len(folds)))\n",
    "\n",
    "transforms_train, transforms_val = get_transforms(IMG_SIZE)\n",
    "\n",
    "oof_preds_all = []\n",
    "oof_targets_all = []\n",
    "oof_names_all = []\n",
    "oof_folds_all = []\n",
    "test_preds_all = np.zeros((len(test_df), num_classes, len(folds)))\n",
    "\n",
    "for fold in folds:\n",
    "        oof_preds, oof_targets, oof_names, oof_folds, test_preds = run(fold, df, root_dir, test_df, transforms_train, transforms_val, num_workers,n_epochs=n_epochs)\n",
    "        oof_preds_all.append(oof_preds)\n",
    "        oof_targets_all.append(oof_targets)\n",
    "        oof_names_all.append(oof_names)\n",
    "        oof_folds_all.append(oof_folds)\n",
    "        test_preds_all[:, :, fold] = test_preds\n",
    "    \n",
    "# Concatenate OOF data\n",
    "oof_preds_all = np.concatenate(oof_preds_all)\n",
    "oof_targets_all = np.concatenate(oof_targets_all)\n",
    "oof_names_all = np.concatenate(oof_names_all)\n",
    "oof_folds_all = np.concatenate(oof_folds_all)\n",
    "\n",
    "# Compute overall OOF AUC\n",
    "auc = roc_auc_score(oof_targets_all, oof_preds_all, multi_class='ovr')\n",
    "print(f'Overall OOF AUC = {auc:.3f}')\n",
    "\n",
    "# Save OOF to CSV\n",
    "df_oof = pd.DataFrame({\n",
    "    'image_name': oof_names_all,\n",
    "    'target': oof_targets_all,\n",
    "    'pred': oof_preds_all.argmax(axis=1),\n",
    "    'fold': oof_folds_all\n",
    "})\n",
    "df_oof.to_csv('oof_8.csv', index=False)\n",
    "print(\"OOF saved to 'oof_8.csv'\")\n",
    "print(df_oof.head())\n",
    "\n",
    "# Average test predictions across 5 folds\n",
    "test_preds_final = test_preds_all.mean(axis=2)  # [n_test, 101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
