{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np \n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2 #np.array -> torch.tensor\n",
    "from pytorch_metric_learning import losses, miners\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "\n",
    "import warnings\n",
    "import itertools\n",
    "import random\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define configs (Miner, Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "  def __init__(self, mean, std):\n",
    "    self.mean = mean\n",
    "    self.std = std\n",
    "\n",
    "  def __call__(self, tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      tensor (Tensor): Tensor image of size (C, H, W) to be normalized'\n",
    "    Returns:\n",
    "      Tensor: Normalized image\n",
    "    \"\"\"\n",
    "    for t, m, s in zip(tensor, self.mean, self.std):\n",
    "      t.mul_(s).add_(m) #in-place operation (not make a copy but change object directly)\n",
    "      #The normalize code -> t.sub_(m).div_(s)\n",
    "    return tensor\n",
    "\n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "configs = load_config('../configs/config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            is_train (bool): Whether this is training set or test set\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Define class folders\n",
    "        self.class_folders = ['normal', 'preplus', 'plus']\n",
    "        \n",
    "        # Create a list of (image_path, class_label) tuples\n",
    "        self.samples = []\n",
    "        for class_idx, class_name in enumerate(self.class_folders):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                        img_path = os.path.join(class_dir, img_name)\n",
    "                        self.samples.append((img_path, class_idx))\n",
    "        \n",
    "        # Shuffle samples\n",
    "        random.shuffle(self.samples)\n",
    "        \n",
    "        # Split into train/test if needed\n",
    "        if not is_train:\n",
    "            # Use last 20% for testing\n",
    "            split_idx = int(len(self.samples) * 0.8)\n",
    "            self.samples = self.samples[split_idx:]\n",
    "        else:\n",
    "            # Use first 80% for training\n",
    "            split_idx = int(len(self.samples) * 0.8)\n",
    "            self.samples = self.samples[:split_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Read image\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(is_train=True):\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss and miner functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_miner(config):\n",
    "    # Initialize miner\n",
    "    if 'triplet_margin_miner' in config['mining']:\n",
    "        miner_config = config['mining']['triplet_margin_miner'][0]  # Use first config\n",
    "        miner = miners.TripletMarginMiner(\n",
    "            margin=miner_config['m'],\n",
    "            type_of_triplets=\"all\"\n",
    "        )\n",
    "    elif 'batch_easy_hard_miner' in config['mining']:\n",
    "        miner_config = config['mining']['batch_easy_hard_miner'][0]  # Use first config\n",
    "        miner = miners.BatchEasyHardMiner(\n",
    "            pos_strategy=miner_config['pos_strategy'],\n",
    "            neg_strategy=miner_config['neg_strategy']\n",
    "        )\n",
    "    else:\n",
    "        miner = None\n",
    "    \n",
    "    # Initialize loss\n",
    "    if 'triplet_loss' in config['loss']:\n",
    "        loss_fn = losses.TripletMarginLoss(margin=0.2)\n",
    "    elif 'ntxent_loss' in config['loss']:\n",
    "        loss_config = config['loss']['ntxent_loss']\n",
    "        loss_fn = losses.NTXentLoss(temperature=loss_config['temperature'])\n",
    "    else:\n",
    "        loss_fn = losses.TripletMarginLoss(margin=0.2)  # Default\n",
    "    \n",
    "    return loss_fn, miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = nn.functional.normalize(x, p=2, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_loaders(configs):\n",
    "    # Define dataset paths - adjust these paths as needed\n",
    "    data_root = '../../ROP-o/Triplet-data/data'\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TripletDataset(\n",
    "        root_dir=data_root,\n",
    "        transform=get_transforms(is_train=True),\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = TripletDataset(\n",
    "        root_dir=data_root,\n",
    "        transform=get_transforms(is_train=False),\n",
    "        is_train=False\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=configs['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=os.cpu_count() or 4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=configs['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=os.cpu_count() or 4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "batch_size = configs['training']['batch_size']\n",
    "n_workers = os.cpu_count()\n",
    "print(\"num_workers =\", n_workers)\n",
    "\n",
    "trainloader, testloader = setup_data_loaders(configs)\n",
    "\n",
    "# model\n",
    "model = EmbeddingNet().to(device)\n",
    "\n",
    "# loss, miner\n",
    "loss_fn, miner = get_loss_and_miner(configs)\n",
    "\n",
    "# optimizer \n",
    "optimizer = optim.Adam(model.parameters(), lr=configs['training']['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, loss_fn, miner, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        embeddings = model(data)\n",
    "        \n",
    "        # Get triplets using miner if available\n",
    "        if miner:\n",
    "            hard_pairs = miner(embeddings, labels)\n",
    "            loss = loss_fn(embeddings, labels, hard_pairs)\n",
    "        else:\n",
    "            loss = loss_fn(embeddings, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        losses.update(loss.item(), data.size(0))\n",
    "    \n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, loss_fn, miner, device):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            embeddings = model(data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            if miner:\n",
    "                hard_pairs = miner(embeddings, labels)\n",
    "                loss = loss_fn(embeddings, labels, hard_pairs)\n",
    "            else:\n",
    "                loss = loss_fn(embeddings, labels)\n",
    "            \n",
    "            # Update statistics\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "    \n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = configs['training']['epochs']\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for e in range(epochs):\n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch(model, trainloader, optimizer, loss_fn, miner, device)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss = evaluate(model, testloader, loss_fn, miner, device)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f'Epoch {e+1}/{epochs}:')\n",
    "    print(f'  Train Loss: {train_loss:.4f}')\n",
    "    print(f'  Test Loss: {test_loss:.4f}')\n",
    "    \n",
    "    # Save best model\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        # Save model checkpoint\n",
    "        checkpoint_dir = '../log/checkpoints'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': e,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "        }, os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "        print(f'  Saved new best model with loss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(model, test_loader, device, n_samples=100):\n",
    "    \"\"\"Visualize embeddings using t-SNE\"\"\"\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    model.eval()\n",
    "    embeddings_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            if len(embeddings_list) >= n_samples:\n",
    "                break\n",
    "                \n",
    "            data = data.to(device)\n",
    "            embeddings = model(data)\n",
    "            \n",
    "            embeddings_list.append(embeddings.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "    \n",
    "    # Concatenate all embeddings and labels\n",
    "    embeddings_array = np.vstack(embeddings_list)\n",
    "    labels_array = np.concatenate(labels_list)\n",
    "    \n",
    "    # Limit to n_samples\n",
    "    if len(embeddings_array) > n_samples:\n",
    "        embeddings_array = embeddings_array[:n_samples]\n",
    "        labels_array = labels_array[:n_samples]\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, label in enumerate(['normal', 'preplus', 'plus']):\n",
    "        mask = labels_array == i\n",
    "        plt.scatter(\n",
    "            embeddings_2d[mask, 0],\n",
    "            embeddings_2d[mask, 1],\n",
    "            label=label,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('t-SNE visualization of embeddings')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "checkpoint_path = '../log/checkpoints/best_model.pth'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    # Visualize embeddings\n",
    "    visualize_embeddings(model, testloader, device)\n",
    "else:\n",
    "    print(\"No saved model found. Using the last trained model.\")\n",
    "    visualize_embeddings(model, testloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
