{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np \n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2 #np.array -> torch.tensor\n",
    "from pytorch_metric_learning import losses, miners\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "\n",
    "import warnings\n",
    "import itertools\n",
    "import random\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define configs (Miner, Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "  def __init__(self, mean, std):\n",
    "    self.mean = mean\n",
    "    self.std = std\n",
    "\n",
    "  def __call__(self, tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      tensor (Tensor): Tensor image of size (C, H, W) to be normalized'\n",
    "    Returns:\n",
    "      Tensor: Normalized image\n",
    "    \"\"\"\n",
    "    for t, m, s in zip(tensor, self.mean, self.std):\n",
    "      t.mul_(s).add_(m) #in-place operation (not make a copy but change object directly)\n",
    "      #The normalize code -> t.sub_(m).div_(s)\n",
    "    return tensor\n",
    "\n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "configs = load_config('../configs/config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            is_train (bool): Whether this is training set or test set\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Define class folders\n",
    "        self.class_folders = ['normal', 'preplus', 'plus']\n",
    "        \n",
    "        # Create a list of (image_path, class_label) tuples\n",
    "        self.samples = []\n",
    "        for class_idx, class_name in enumerate(self.class_folders):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                        img_path = os.path.join(class_dir, img_name)\n",
    "                        self.samples.append((img_path, class_idx))\n",
    "        \n",
    "        # Shuffle samples\n",
    "        random.shuffle(self.samples)\n",
    "        \n",
    "        # Split into train/test if needed\n",
    "        if not is_train:\n",
    "            # Use last 20% for testing\n",
    "            split_idx = int(len(self.samples) * 0.8)\n",
    "            self.samples = self.samples[split_idx:]\n",
    "        else:\n",
    "            # Use first 80% for training\n",
    "            split_idx = int(len(self.samples) * 0.8)\n",
    "            self.samples = self.samples[:split_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Read image\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(is_train=True):\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss and miner functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_miner(config):\n",
    "    # Initialize miner\n",
    "    if 'triplet_margin_miner' in config['mining']:\n",
    "        miner_config = config['mining']['triplet_margin_miner'][0]  # Use first config\n",
    "        miner = miners.TripletMarginMiner(\n",
    "            margin=miner_config['m'],\n",
    "            type_of_triplets=\"all\"\n",
    "        )\n",
    "    elif 'batch_easy_hard_miner' in config['mining']:\n",
    "        miner_config = config['mining']['batch_easy_hard_miner'][0]  # Use first config\n",
    "        miner = miners.BatchEasyHardMiner(\n",
    "            pos_strategy=miner_config['pos_strategy'],\n",
    "            neg_strategy=miner_config['neg_strategy']\n",
    "        )\n",
    "    else:\n",
    "        miner = None\n",
    "    \n",
    "    # Initialize loss\n",
    "    if 'triplet_loss' in config['loss']:\n",
    "        loss_fn = losses.TripletMarginLoss(margin=0.2)\n",
    "    elif 'ntxent_loss' in config['loss']:\n",
    "        loss_config = config['loss']['ntxent_loss']\n",
    "        loss_fn = losses.NTXentLoss(temperature=loss_config['temperature'])\n",
    "    else:\n",
    "        loss_fn = losses.TripletMarginLoss(margin=0.2)  # Default\n",
    "    \n",
    "    return loss_fn, miner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = nn.functional.normalize(x, p=2, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_loaders(configs):\n",
    "    # Define dataset paths - adjust these paths as needed\n",
    "    data_root = '../../ROP-o/Triplet-data/data'\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TripletDataset(\n",
    "        root_dir=data_root,\n",
    "        transform=get_transforms(is_train=True),\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = TripletDataset(\n",
    "        root_dir=data_root,\n",
    "        transform=get_transforms(is_train=False),\n",
    "        is_train=False\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=configs['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=os.cpu_count() or 4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=configs['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=os.cpu_count() or 4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_workers = 16\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m n_workers \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mcpu_count()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers =\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_workers)\n\u001b[1;32m----> 9\u001b[0m trainloader, testloader \u001b[38;5;241m=\u001b[39m \u001b[43msetup_data_loaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m EmbeddingNet()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m, in \u001b[0;36msetup_data_loaders\u001b[1;34m(configs)\u001b[0m\n\u001b[0;32m     12\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m TripletDataset(\n\u001b[0;32m     13\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39mdata_root,\n\u001b[0;32m     14\u001b[0m     transform\u001b[38;5;241m=\u001b[39mget_transforms(is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m     15\u001b[0m     is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Create data loaders\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     28\u001b[0m     test_dataset,\n\u001b[0;32m     29\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mconfigs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loader, test_loader\n",
      "File \u001b[1;32mc:\\Users\\UCL\\AppData\\Local\\anaconda3\\envs\\Do_ROP\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:376\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 376\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\UCL\\AppData\\Local\\anaconda3\\envs\\Do_ROP\\lib\\site-packages\\torch\\utils\\data\\sampler.py:164\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load data\n",
    "batch_size = configs['training']['batch_size']\n",
    "n_workers = os.cpu_count()\n",
    "print(\"num_workers =\", n_workers)\n",
    "\n",
    "trainloader, testloader = setup_data_loaders(configs)\n",
    "\n",
    "# model\n",
    "model = EmbeddingNet().to(device)\n",
    "\n",
    "# loss, miner\n",
    "loss_fn, miner = get_loss_and_miner(configs)\n",
    "\n",
    "# optimizer \n",
    "optimizer = optim.Adam(model.parameters(), lr=configs['training']['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, loss_fn, miner, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        embeddings = model(data)\n",
    "        \n",
    "        # Get triplets using miner if available\n",
    "        if miner:\n",
    "            hard_pairs = miner(embeddings, labels)\n",
    "            loss = loss_fn(embeddings, labels, hard_pairs)\n",
    "        else:\n",
    "            loss = loss_fn(embeddings, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        losses.update(loss.item(), data.size(0))\n",
    "    \n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, loss_fn, miner, device):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            embeddings = model(data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            if miner:\n",
    "                hard_pairs = miner(embeddings, labels)\n",
    "                loss = loss_fn(embeddings, labels, hard_pairs)\n",
    "            else:\n",
    "                loss = loss_fn(embeddings, labels)\n",
    "            \n",
    "            # Update statistics\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "    \n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = configs['training']['epochs']\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for e in range(epochs):\n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch(model, trainloader, optimizer, loss_fn, miner, device)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss = evaluate(model, testloader, loss_fn, miner, device)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f'Epoch {e+1}/{epochs}:')\n",
    "    print(f'  Train Loss: {train_loss:.4f}')\n",
    "    print(f'  Test Loss: {test_loss:.4f}')\n",
    "    \n",
    "    # Save best model\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        # Save model checkpoint\n",
    "        checkpoint_dir = '../log/checkpoints'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': e,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "        }, os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "        print(f'  Saved new best model with loss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(model, test_loader, device, n_samples=100):\n",
    "    \"\"\"Visualize embeddings using t-SNE\"\"\"\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    model.eval()\n",
    "    embeddings_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            if len(embeddings_list) >= n_samples:\n",
    "                break\n",
    "                \n",
    "            data = data.to(device)\n",
    "            embeddings = model(data)\n",
    "            \n",
    "            embeddings_list.append(embeddings.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "    \n",
    "    # Concatenate all embeddings and labels\n",
    "    embeddings_array = np.vstack(embeddings_list)\n",
    "    labels_array = np.concatenate(labels_list)\n",
    "    \n",
    "    # Limit to n_samples\n",
    "    if len(embeddings_array) > n_samples:\n",
    "        embeddings_array = embeddings_array[:n_samples]\n",
    "        labels_array = labels_array[:n_samples]\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, label in enumerate(['normal', 'preplus', 'plus']):\n",
    "        mask = labels_array == i\n",
    "        plt.scatter(\n",
    "            embeddings_2d[mask, 0],\n",
    "            embeddings_2d[mask, 1],\n",
    "            label=label,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('t-SNE visualization of embeddings')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "checkpoint_path = '../log/checkpoints/best_model.pth'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1} with loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    # Visualize embeddings\n",
    "    visualize_embeddings(model, testloader, device)\n",
    "else:\n",
    "    print(\"No saved model found. Using the last trained model.\")\n",
    "    visualize_embeddings(model, testloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Do_ROP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
